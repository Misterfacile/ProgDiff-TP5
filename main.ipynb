{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim.optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jeux de Données linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération du jeu de donn ́ees lin ́eaire\n",
    "np.random.seed(0)\n",
    "n_samples = 100\n",
    "x_linear = np.linspace(-10, 10, n_samples)\n",
    "y_linear = 3 * x_linear + 5 + np.random.normal(0, 2,\n",
    "n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jeux de Données non-linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération du jeu de données non linéaire\n",
    "y_nonlinear = 0.5 * x_linear**2 - 4 * x_linear + np.random. normal(0, 5, n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot datasets\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x_linear, y_linear, label='Linear Dataset')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x_linear, y_nonlinear, label='Non-Linear Dataset', color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Présentation des Algorithmes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD est une méthode d’optimisation qui met à jour les paramètres du modèle en fonction du gradient de la fonction de perte par rapport aux paramètres.\n",
    "\n",
    "Pour implémenter l'optimizer selon la formule donnée par l'énoncé, il faudra d'abord parcourir les paramètres passés en input et skip si aucun gradient n'est disponible puis ensuite appliquer la formule en multipliant la data par le learning_rate en le soustraire par l'ancien gradient précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.01):\n",
    "        if learning_rate <= 0.0 :\n",
    "            raise ValueError(\"Learning rate must be positive\")\n",
    "        \n",
    "        defaults = {'learning_rate': learning_rate}\n",
    "        super(SGD, self).__init__(params, defaults)\n",
    "    \n",
    "    def step(self, closure=None):\n",
    "        # Iterate through parameter groups\n",
    "        for group in self.param_groups:\n",
    "            learning_rate = group['learning_rate']\n",
    "            \n",
    "            # Iterate through parameters in the group\n",
    "            for param in group['params']:\n",
    "                # Skip if no gradient is available\n",
    "                if param.grad is None:\n",
    "                    continue  \n",
    "\n",
    "                # Update the parameter using the gradient\n",
    "                param.data -= learning_rate * param.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSProp adapte le taux d’apprentissage pour chaque paramètre en fonction de la moyenne des carrés des gradients passés\n",
    "\n",
    "Nous allons donc set les variables learning_rate et decay.\n",
    "Une variable d'état square_avg est créée si elle n'existe pas déjà pour stocker la moyenne des carrés des gradients. Cela permet de suivre l'historique des gradients pour chaque paramètre.\n",
    "\n",
    "Celui-ci est update avec le facteur décroissance decay qui contrôle l'importance des gradients passés.\n",
    "\n",
    "La racine carrée de square_avg est utilisée pour normaliser l'amplitude des gradients.\n",
    "Epsilon rajouté car se trouvant dans la formule pour la stabilité de celle-ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.01, decay=0.9, epsilon=1e-8):\n",
    "        if learning_rate <= 0.0:\n",
    "            raise ValueError(\"Learning rate must be positive\")\n",
    "        if not 0.0 <= decay < 1.0:\n",
    "            raise ValueError(\"Decay must be positive and less than 1\")\n",
    "        \n",
    "        defaults = {\n",
    "            'learning_rate': learning_rate,\n",
    "            'decay': decay,\n",
    "            'epsilon': epsilon\n",
    "        }\n",
    "        super(RMSProp, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        for group in self.param_groups:\n",
    "            learning_rate = group['learning_rate']\n",
    "            decay = group['decay']\n",
    "            epsilon = group['epsilon']\n",
    "            \n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                grad = param.grad.data\n",
    "                \n",
    "                # Initialize state for square_avg\n",
    "                state = self.state[param]\n",
    "                if 'square_avg' not in state:\n",
    "                    state['square_avg'] = torch.zeros_like(param.data)\n",
    "                \n",
    "                square_avg = state['square_avg']\n",
    "                \n",
    "                # Update moving average of squared gradients\n",
    "                square_avg.mul_(decay).addcmul_(grad, grad, value=1 - decay)\n",
    "                \n",
    "                # Update parameters\n",
    "                param.data -= learning_rate * grad / (square_avg.sqrt() + epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adagrad ajuste le taux d’apprentissage pour chaque paramètre en fonction des gradients pass ́es, favorisant des mises à jour plus importantes pour les paramètres moins fréquemment mis à jour.\n",
    "\n",
    "L'état de chaque paramètre conserve une variable sum_of_squares, représentant la somme cumulative des carrés des gradients Gt​.\n",
    "Les mises à jour sont normalisées en divisant par la racine carré de Gt​+ϵ ​, ce qui réduit l'importance des mises à jour pour des dimensions avec de grands gradients cumulatifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.01, epsilon=1e-10):\n",
    "        if learning_rate <= 0.0:\n",
    "            raise ValueError(\"Learning rate must be positive\")\n",
    "        \n",
    "        if epsilon <= 0.0:\n",
    "            raise ValueError(\"Epsilon must be positive\")\n",
    "        \n",
    "        defaults = {\n",
    "            'learning_rate': learning_rate,\n",
    "            'epsilon': epsilon\n",
    "        }\n",
    "        super(Adagrad, self).__init__(params, defaults)\n",
    "    \n",
    "    def step(self, closure=None):\n",
    "        for group in self.param_groups:\n",
    "            learning_rate = group['learning_rate']\n",
    "            epsilon = group['epsilon']\n",
    "            \n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                grad = param.grad.data\n",
    "                \n",
    "                # Initialiser le Gt pour chaque paramètre\n",
    "                if 'sum_of_squares' not in self.state[param]:\n",
    "                    self.state[param]['sum_of_squares'] = torch.zeros_like(grad)\n",
    "                \n",
    "                sum_of_squares = self.state[param]['sum_of_squares']\n",
    "                \n",
    "                # Update la somme des carrés des gradients\n",
    "                sum_of_squares.addcmul_(grad, grad, value=1)\n",
    "                \n",
    "                # Update les paramètres en utilisant la formule\n",
    "                param.data -= learning_rate * grad / (sum_of_squares.sqrt() + epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam combine les avantages de RMSProp et de Momentum en utilisant des moments exponentiels pour estimer les premiers et seconds moments des gradients.\n",
    "\n",
    "Les paramètres :\n",
    "- learning_rate (η) : Taux d'apprentissage.\n",
    "- beta1 (β1) : Facteur de pondération pour le premier moment (mtmt​).\n",
    "- beta2 (β2) : Facteur de pondération pour le second moment (vtvt​).\n",
    "- epsilon (ϵ) : Petit terme ajouté pour éviter les divisions par zéro.\n",
    "\n",
    "Tout d'abord, on initialise les moments avec les variables m, v, ensuite avec la formule donnée dans l'énoncé on calcule \n",
    "les moments exponentiels en combinant les valeurs précédentes (mt−1​, vt−1​) avec les gradients actuels (∇θL(θt)).\n",
    "Après ça pour éviter les biais initiaux (lorsque m0=0 et v0=0), les moments sont corrigés.\n",
    "Pour finir, on update les paramètres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        defaults = {\n",
    "            'learning_rate': learning_rate,\n",
    "            'beta1': beta1,\n",
    "            'beta2': beta2,\n",
    "            'epsilon': epsilon\n",
    "        }\n",
    "        super(Adam, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        for group in self.param_groups:\n",
    "            learning_rate = group['learning_rate']\n",
    "            beta1 = group['beta1']\n",
    "            beta2 = group['beta2']\n",
    "            epsilon = group['epsilon']\n",
    "\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                grad = param.grad.data\n",
    "                if param not in self.state:\n",
    "                    self.state[param] = {\n",
    "                        'step': 0,\n",
    "                        'm': torch.zeros_like(param.data),  # Premier moment\n",
    "                        'v': torch.zeros_like(param.data)   # Second moment\n",
    "                    }\n",
    "\n",
    "                state = self.state[param]\n",
    "                m, v = state['m'], state['v']\n",
    "\n",
    "                # Incrémenter le compteur de pas\n",
    "                state['step'] += 1\n",
    "                step = state['step']\n",
    "\n",
    "                # Update des moments exponentiels\n",
    "                m.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                v.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                # Calcululate des moments corrigés par le biais\n",
    "                m_hat = m / (1 - beta1 ** step)\n",
    "                v_hat = v / (1 - beta2 ** step)\n",
    "\n",
    "                # Update des paramètres\n",
    "                param.data.addcdiv_(-learning_rate * m_hat, (v_hat.sqrt() + epsilon))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdamW est une variante d’Adam qui ajoute une régularisation L2 pour améliorer la généralisation.\n",
    "\n",
    "Même chose que Adam optimizer sauf que l'on ajoute la régularization qui est le weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW(Optimizer):\n",
    "    def __init__(self, params, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01):\n",
    "        defaults = {\n",
    "            'learning_rate': learning_rate,\n",
    "            'beta1': beta1,\n",
    "            'beta2': beta2,\n",
    "            'epsilon': epsilon,\n",
    "            'weight_decay': weight_decay\n",
    "        }\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        for group in self.param_groups:\n",
    "            learning_rate = group['learning_rate']\n",
    "            beta1 = group['beta1']\n",
    "            beta2 = group['beta2']\n",
    "            epsilon = group['epsilon']\n",
    "            weight_decay = group['weight_decay']\n",
    "\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                grad = param.grad.data\n",
    "\n",
    "                if param not in self.state:\n",
    "                    self.state[param] = {\n",
    "                        'step': 0,\n",
    "                        'm': torch.zeros_like(param.data),  # Premier moment\n",
    "                        'v': torch.zeros_like(param.data)   # Second moment\n",
    "                    }\n",
    "\n",
    "                state = self.state[param]\n",
    "                m, v = state['m'], state['v']\n",
    "\n",
    "                state['step'] += 1\n",
    "                step = state['step']\n",
    "\n",
    "                # Update biased first and second moment estimates\n",
    "                m.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                v.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                 # Calcululate des moments corrigés par le biais\n",
    "                m_hat = m / (1 - beta1 ** step)\n",
    "                v_hat = v / (1 - beta2 ** step)\n",
    "\n",
    "                # Update des paramètres avec weight decay (L2 regularization)\n",
    "                param.data.add_(-learning_rate * weight_decay, param.data)  #  weight decay\n",
    "                param.data.addcdiv_(-learning_rate * m_hat, (v_hat.sqrt() + epsilon))  #  Adam update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation des optimiseurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x - 2)**2\n",
    "def f_nonconvexe(x): \n",
    "    return 3*x**2 - 2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on Convex Loss Function (x, loss):\n",
      "{'SGD': (1.976941466331482, 0.000830773264169693), 'RMSProp': (1.8312522172927856, 0.03999539464712143), 'Adagrad': (0.6596569418907166, 1.8502353429794312), 'Adam': (1.5680800676345825, 0.2586780786514282), 'AdamW': (1.4806982278823853, 0.3574395477771759)}\n",
      "\n",
      "Evaluating on Non-Convex Loss Function (x, loss):\n",
      "{'SGD': (0.33333298563957214, -0.3333333134651184), 'RMSProp': (0.33333295583724976, -0.3333333134651184), 'Adagrad': (0.14167165756225586, -0.21363523602485657), 'Adam': (0.590474545955658, -0.14301443099975586), 'AdamW': (0.5916758179664612, -0.1528463363647461)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_t/_09rqyyj0v3g_c9sgq8sp4k40000gn/T/ipykernel_2869/855372235.py:48: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1630.)\n",
      "  param.data.add_(-learning_rate * weight_decay, param.data)  #  weight decay\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function for optimizers\n",
    "def eval_optim():\n",
    "    # Definit x comme le paramètres initial comme a learnable tensor)\n",
    "    x = torch.tensor([0.0], requires_grad=True)  \n",
    "\n",
    "    # List of optimizers to evaluate\n",
    "    optimizers = {\n",
    "        \"SGD\": SGD([x], learning_rate=0.1),\n",
    "        \"RMSProp\": RMSProp([x], learning_rate=0.1),\n",
    "        \"Adagrad\": Adagrad([x], learning_rate=0.1),\n",
    "        \"Adam\": Adam([x], learning_rate=0.1),\n",
    "        \"AdamW\": AdamW([x], learning_rate=0.1, weight_decay=0.01)\n",
    "    }\n",
    "\n",
    "    # Loss functions to evaluate\n",
    "    loss_functions = {\n",
    "        \"Convex\": f,\n",
    "        \"Non-Convex\": f_nonconvexe\n",
    "    }\n",
    "\n",
    "    result = {}\n",
    "    step = 20\n",
    "    for loss_name, loss_fn in loss_functions.items():\n",
    "        print(f\"\\nEvaluating on {loss_name} Loss Function (x, loss):\")\n",
    "        \n",
    "        for optim_name, optimizer in optimizers.items():\n",
    "            # Reset parameter x to 0 for each optimizer\n",
    "            x.data = torch.tensor([0.0])\n",
    "\n",
    "            # print(f\"\\nOptimizer: {optim_name}\")\n",
    "            for step in range(step):  \n",
    "                # On reset le gradient\n",
    "                optimizer.zero_grad() \n",
    "                \n",
    "                # Compute la loss \n",
    "                loss = loss_fn(x)\n",
    "                \n",
    "                # Compute gradients\n",
    "                loss.backward()\n",
    "                \n",
    "                # Update x\n",
    "                optimizer.step()\n",
    "\n",
    "                # Print progress\n",
    "                # print(f\"Step {step + 1:02d}: x = {x.item():.4f}, Loss = {loss.item():.4f}\")\n",
    "            result[optim_name] = (x.item(), loss.item())\n",
    "        print(result)\n",
    "            \n",
    "eval_optim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseaux de Neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_nn(x, W1, b1, W2, b2): \n",
    "    h1 = W1 * x + b1\n",
    "    y = W2 * h1 + b2 \n",
    "    return y\n",
    "\n",
    "def mse(y, y_hat):\n",
    "    return (y - y_hat) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction eval_nn_optim est conçue pour évaluer et comparer les performances de différents algorithmes d’optimisation dans un contexte d’apprentissage supervisé. Elle commence par générer des données simples représentant une relation linéaire y=3x+2, avec des entrées uniformément réparties. Ces données servent à entraîner un réseau de neurones à une seule couche cachée, dont les paramètres (W1,W2,b1,b2) sont initialisés aléatoirement avec des gradients activés.\n",
    "\n",
    "Les optimiseurs testés incluent des méthodes classiques comme SGD, RMSProp, Adagrad, Adam et AdamW, chacune configurée avec un taux d’apprentissage spécifique. Pour chaque optimiseur, la fonction réinitialise les paramètres du modèle avant de procéder à un entraînement de 100 itérations. À chaque étape, la fonction calcule les prédictions du réseau, évalue la perte à l’aide de la fonction d’erreur quadratique moyenne MSE, et met à jour les paramètres via l’optimiseur choisi.\n",
    "\n",
    "À la fin de l’entraînement, la perte finale pour chaque optimiseur est enregistrée dans un dictionnaire et affichée. Cela permet d’identifier l’optimiseur qui converge le mieux pour minimiser la perte. Cette fonction est utile pour comprendre les différences entre les algorithmes d’optimisation et leur efficacité dans un cadre simple, bien que les résultats puissent varier selon les hyperparamètres ou la complexité du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SGD': 7.258194154209641e-14, 'RMSProp': 0.028666885569691658, 'Adagrad': 0.09497125446796417, 'Adam': 5.608999344985932e-05, 'AdamW': 0.00011092250497313216}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'SGD': 7.258194154209641e-14,\n",
       " 'RMSProp': 0.028666885569691658,\n",
       " 'Adagrad': 0.09497125446796417,\n",
       " 'Adam': 5.608999344985932e-05,\n",
       " 'AdamW': 0.00011092250497313216}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entraînement du réseau\n",
    "def eval_nn_optim():\n",
    "    # Génération de données simples\n",
    "    x = torch.linspace(-1, 1, 100)  # Entrées\n",
    "    y_true = 3 * x + 2  # Sorties cibles (relation linéaire)\n",
    "\n",
    "    # Initialisation des paramètres du réseau\n",
    "    W1 = torch.randn(1, requires_grad=True)\n",
    "    b1 = torch.randn(1, requires_grad=True)\n",
    "    W2 = torch.randn(1, requires_grad=True)\n",
    "    b2 = torch.randn(1, requires_grad=True)\n",
    "\n",
    "    # Liste des optimiseurs à tester\n",
    "    optimizers = {\n",
    "        \"SGD\": SGD([W1, b1, W2, b2], learning_rate=0.1),\n",
    "        \"RMSProp\": RMSProp([W1, b1, W2, b2], learning_rate=0.1),\n",
    "        \"Adagrad\": Adagrad([W1, b1, W2, b2], learning_rate=0.1),\n",
    "        \"Adam\": Adam([W1, b1, W2, b2], learning_rate=0.1),\n",
    "        \"AdamW\": AdamW([W1, b1, W2, b2], learning_rate=0.1, weight_decay=0.01)\n",
    "    }\n",
    "\n",
    "    step = 100\n",
    "    result = {}\n",
    "    \n",
    "    # Boucle d'évaluation\n",
    "    for optim_name, optimizer in optimizers.items():\n",
    "        # print(f\"\\nOptimiseur : {optim_name}\")\n",
    "\n",
    "        # Réinitialisation des paramètres\n",
    "        W1.data = torch.randn(1)\n",
    "        b1.data = torch.randn(1)\n",
    "        W2.data = torch.randn(1)\n",
    "        b2.data = torch.randn(1)\n",
    "\n",
    "        # Training of steps\n",
    "        for epoch in range(step):\n",
    "            optimizer.zero_grad()  # Reinitialize the gradients\n",
    "\n",
    "            # Calculate the prediction\n",
    "            y_pred = func_nn(x, W1, b1, W2, b2)\n",
    "\n",
    "            # loss calculation\n",
    "            loss = mse(y_true, y_pred).mean()\n",
    "            loss.backward()  # Gradient calculation\n",
    "            optimizer.step()  # Update the parameters\n",
    "\n",
    "            # Display the epoch\n",
    "            # if (epoch + 1) % 10 == 0:\n",
    "            #     print(f\"Epoch {epoch + 1}: Loss = {loss.item():.4f}\")\n",
    "                \n",
    "        result[optim_name] = loss.item()\n",
    "    print(result)\n",
    "    return result\n",
    "eval_nn_optim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduler de Taux d'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LRScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe LRScheduler permet de gérer cette réduction progressive du taux d'apprentissage en multipliant ce dernier par 0.95 à chaque étape de l'entraînement. Elle est initialisée avec un optimiseur et un taux d'apprentissage initial, et offre une méthode step pour effectuer la réduction, ainsi qu'une méthode get_lr pour récupérer le taux d'apprentissage actuel de l'optimiseur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRScheduler:\n",
    "    def __init__(self, optimizer, initial_lr):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "        \n",
    "        # Init du taux d'apprentissage de l'optimiseur\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['learning_rate'] = self.initial_lr\n",
    "\n",
    "    def step(self):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['learning_rate'] *= 0.95 # Réduction du taux d'apprentissage de 5%\n",
    "\n",
    "    def get_lr(self):\n",
    "        # Return le taux d'apprentissage actuel\n",
    "        return [param_group['learning_rate'] for param_group in self.optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On reprend la fonction eval_optim coder ci-dessus et ajouter le lrscheduler en plus. On remarque des résultats qui convergent et plutôt convaincante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on Convex Loss Function (x, loss):\n",
      "{'SGD': (2.0, 0.0), 'RMSProp': (2.0, 0.0), 'Adagrad': (1.9997732639312744, 5.140924486113363e-08), 'Adam': (2.0008790493011475, 7.727276738478395e-07), 'AdamW': (1.995052695274353, 2.4475824830005877e-05)}\n",
      "\n",
      "Evaluating on Non-Convex Loss Function (x, loss):\n",
      "{'SGD': (0.3333333134651184, -0.3333333432674408), 'RMSProp': (0.3333333432674408, -0.3333333134651184), 'Adagrad': (0.3333333432674408, -0.3333333134651184), 'Adam': (0.3332196772098541, -0.333333283662796), 'AdamW': (0.333037406206131, -0.3333330750465393)}\n"
     ]
    }
   ],
   "source": [
    "def eval_optim_lrscheduler():\n",
    "    # Definit x comme le paramètres initial comme a learnable tensor)\n",
    "    x = torch.tensor([0.0], requires_grad=True)  \n",
    "    \n",
    "    optimizers = {\n",
    "        \"SGD\": SGD([x], learning_rate=0.1),\n",
    "        \"RMSProp\": RMSProp([x], learning_rate=0.1),\n",
    "        \"Adagrad\": Adagrad([x], learning_rate=0.1),\n",
    "        \"Adam\": Adam([x], learning_rate=0.1),\n",
    "        \"AdamW\": AdamW([x], learning_rate=0.1, weight_decay=0.01)\n",
    "    }\n",
    "\n",
    "    # Loss functions to evaluate\n",
    "    loss_functions = {\n",
    "        \"Convex\": f,\n",
    "        \"Non-Convex\": f_nonconvexe\n",
    "    }\n",
    "\n",
    "    result = {}\n",
    "    step = 300\n",
    "    for loss_name, loss_fn in loss_functions.items():\n",
    "        print(f\"\\nEvaluating on {loss_name} Loss Function (x, loss):\")\n",
    "        \n",
    "        for optim_name, optimizer in optimizers.items():\n",
    "            # Reset parameter x to 0 for each optimizer\n",
    "            x.data = torch.tensor([0.0])\n",
    "            scheduler = LRScheduler(optimizer, initial_lr=0.95)\n",
    "\n",
    "            # print(f\"\\nOptimizer: {optim_name}\")\n",
    "            for step in range(step):  \n",
    "                # On reset le gradient\n",
    "                optimizer.zero_grad() \n",
    "                \n",
    "                # Compute la loss \n",
    "                loss = loss_fn(x)\n",
    "                \n",
    "                # Compute gradients\n",
    "                loss.backward()\n",
    "                \n",
    "                # Update x\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "            result[optim_name] = (x.item(), loss.item())\n",
    "        print(result)\n",
    "            \n",
    "eval_optim_lrscheduler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Des résultats plutôt convaincantes mais l'optimizer AdamGrad a du mal à converger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SGD': 3.8055603807096716e-11, 'RMSProp': 5.0285886294808985e-14, 'Adagrad': 1.7795729637145996, 'Adam': 4.751055530505255e-05, 'AdamW': 1.6001977201085538e-06}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'SGD': 3.8055603807096716e-11,\n",
       " 'RMSProp': 5.0285886294808985e-14,\n",
       " 'Adagrad': 1.7795729637145996,\n",
       " 'Adam': 4.751055530505255e-05,\n",
       " 'AdamW': 1.6001977201085538e-06}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entraînement du réseau\n",
    "def eval_nn_optim_lrscheduler():\n",
    "    # Génération de données simples\n",
    "    x = torch.linspace(-1, 1, 100)  # Entrées\n",
    "    y_true = 3 * x + 2  # Sorties cibles (relation linéaire)\n",
    "\n",
    "    # Initialisation des paramètres du réseau\n",
    "    W1 = torch.randn(1, requires_grad=True)\n",
    "    b1 = torch.randn(1, requires_grad=True)\n",
    "    W2 = torch.randn(1, requires_grad=True)\n",
    "    b2 = torch.randn(1, requires_grad=True)\n",
    "\n",
    "    # Liste des optimiseurs à tester\n",
    "    optimizers = {\n",
    "        \"SGD\": SGD([W1, b1, W2, b2], learning_rate=0.1),\n",
    "        \"RMSProp\": RMSProp([W1, b1, W2, b2], learning_rate=0.1),\n",
    "        \"Adagrad\": Adagrad([W1, b1, W2, b2], learning_rate=0.1),\n",
    "        \"Adam\": Adam([W1, b1, W2, b2], learning_rate=0.1),\n",
    "        \"AdamW\": AdamW([W1, b1, W2, b2], learning_rate=0.1, weight_decay=0.01)\n",
    "    }\n",
    "\n",
    "    step = 300\n",
    "    result = {}\n",
    "    \n",
    "    # Boucle d'évaluation\n",
    "    for optim_name, optimizer in optimizers.items():\n",
    "        # print(f\"\\nOptimiseur : {optim_name}\")\n",
    "\n",
    "        # Réinitialisation des paramètres\n",
    "        W1.data = torch.randn(1)\n",
    "        b1.data = torch.randn(1)\n",
    "        W2.data = torch.randn(1)\n",
    "        b2.data = torch.randn(1)\n",
    "        \n",
    "        scheduler = LRScheduler(optimizer, initial_lr=0.2)\n",
    "\n",
    "        # Training of steps\n",
    "        for epoch in range(step):\n",
    "            optimizer.zero_grad()  # Reinitialize the gradients\n",
    "\n",
    "            # Calculate the prediction\n",
    "            y_pred = func_nn(x, W1, b1, W2, b2)\n",
    "\n",
    "            # loss calculation\n",
    "            loss = mse(y_true, y_pred).mean()\n",
    "            loss.backward()  # Gradient calculation\n",
    "            optimizer.step()  # Update the parameters\n",
    "            scheduler.step()\n",
    "\n",
    "            # Display the epoch\n",
    "            # if (epoch + 1) % 10 == 0:\n",
    "            #     print(f\"Epoch {epoch + 1}: Loss = {loss.item():.4f}\")\n",
    "                \n",
    "        result[optim_name] = loss.item()\n",
    "    print(result)\n",
    "    return result\n",
    "eval_nn_optim_lrscheduler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRSchedulerOnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRSchedulerOnPlateau(LRScheduler):\n",
    "    def __init__(self, optimizer, initial_lr, patience=10, factor=0.1, min_lr=1e-6, mode='min', threshold=1e-4):\n",
    "        super().__init__(optimizer, initial_lr)\n",
    "        self.patience = patience\n",
    "        self.factor = factor\n",
    "        self.min_lr = min_lr\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "        self.best_metric = None\n",
    "        self.num_bad_epochs = 0\n",
    "\n",
    "    def step(self, current_metric):\n",
    "        if self.best_metric is None:\n",
    "            self.best_metric = current_metric\n",
    "            return\n",
    "\n",
    "        if self._is_improvement(current_metric):\n",
    "            self.best_metric = current_metric\n",
    "            self.num_bad_epochs = 0\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            self._reduce_lr()\n",
    "            self.num_bad_epochs = 0\n",
    "\n",
    "    def _is_improvement(self, current_metric):\n",
    "        if self.mode == 'min':\n",
    "            return current_metric < self.best_metric - self.threshold\n",
    "        elif self.mode == 'max':\n",
    "            return current_metric > self.best_metric + self.threshold\n",
    "        else:\n",
    "            raise ValueError(\"Mode must be 'min' or 'max'\")\n",
    "\n",
    "    def _reduce_lr(self):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            new_lr = max(param_group['learning_rate'] * self.factor, self.min_lr)\n",
    "            param_group['learning_rate'] = new_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les optimiseurs utilisés incluent SGD, RMSProp, Adagrad, Adam et AdamW, chacun ayant une stratégie différente pour ajuster les gradients. Le planificateur LRSchedulerOnPlateau ajuste le taux d'apprentissage dynamiquement en fonction de l'évolution de la perte. Si la perte ne diminue pas suffisamment pendant 10 itérations consécutives (variable patience), le planificateur réduit le taux d'apprentissage par un facteur de 0,1, tout en respectant une limite inférieure définie par min_lr.\n",
    "\n",
    "Les résultats montrent une performance distincte des optimiseurs pour les deux fonctions de perte. Pour la fonction convexe, Adagrad se distingue avec une convergence presque parfaite vers la solution optimale x≈2x≈2, avec une perte proche de zéro. SGD suit de près mais est légèrement moins précis. RMSProp, Adam, et AdamW convergent moins précisément, avec des pertes plus élevées et des solutions sous-optimales.\n",
    "\n",
    "Pour la fonction non convexe, le comportement change radicalement. Adagrad et RMSProp parviennent à s'approcher très près du minimum global, avec une perte de −0.3333−0.3333, tandis que les autres optimiseurs convergent vers des minima locaux. SGD échoue à converger et diverge, atteignant des valeurs de xx et de perte très élevées. Ce comportement est typique pour des fonctions non convexes où les optimiseurs plus simples, comme SGD, sont sensibles aux plateaux ou aux gradients peu informatifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on Convex Loss Function (x, loss):\n",
      "{'SGD': (1.9997135400772095, 8.205928736515489e-08), 'RMSProp': (2.107083559036255, 0.011467092670500278), 'Adagrad': (1.999996542930603, 1.1951328815484885e-11), 'Adam': (2.2154009342193604, 0.046397872269153595), 'AdamW': (2.1779556274414062, 0.03166845813393593)}\n",
      "\n",
      "Evaluating on Non-Convex Loss Function (x, loss):\n",
      "{'SGD': (928.9714965820312, 2587137.0), 'RMSProp': (0.3333396017551422, -0.3333333134651184), 'Adagrad': (0.3333333432674408, -0.3333333134651184), 'Adam': (0.33527055382728577, -0.33332207798957825), 'AdamW': (0.34196606278419495, -0.3331097662448883)}\n"
     ]
    }
   ],
   "source": [
    "def eval_optim_lrschedulerOnPlateau():\n",
    "    # Define x as the learnable parameter\n",
    "    x = torch.tensor([0.0], requires_grad=True)\n",
    "    optimizers = {\n",
    "        \"SGD\": SGD([x], learning_rate=0.1),\n",
    "        \"RMSProp\": RMSProp([x], learning_rate=0.1),\n",
    "        \"Adagrad\": Adagrad([x], learning_rate=0.1),\n",
    "        \"Adam\": Adam([x], learning_rate=0.1),\n",
    "        \"AdamW\": AdamW([x], learning_rate=0.1, weight_decay=0.01)\n",
    "    }\n",
    "    loss_functions = {\n",
    "        \"Convex\": f,\n",
    "        \"Non-Convex\": f_nonconvexe\n",
    "    }\n",
    "\n",
    "    result = {}\n",
    "    num_steps = 200\n",
    "    for loss_name, loss_fn in loss_functions.items():\n",
    "        print(f\"\\nEvaluating on {loss_name} Loss Function (x, loss):\")\n",
    "        \n",
    "        for optim_name, optimizer in optimizers.items():\n",
    "            # Reset parameter x to 0 for each optimizer\n",
    "            x.data = torch.tensor([0.0])\n",
    "\n",
    "            # Initialize the scheduler with custom parameters\n",
    "            scheduler = LRSchedulerOnPlateau(optimizer, initial_lr=0.95)\n",
    "\n",
    "            for step in range(num_steps):  \n",
    "                optimizer.zero_grad() \n",
    "                loss = loss_fn(x)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Update x\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update the scheduler with the current loss value\n",
    "                scheduler.step(loss.item())\n",
    "\n",
    "                # current_lr = scheduler.get_lr()\n",
    "                # print(f\"Step {step + 1:02d}: Loss = {loss.item():.4f}, Learning Rate = {current_lr}\")\n",
    "\n",
    "            result[optim_name] = (x.item(), loss.item())\n",
    "        print(result)\n",
    "\n",
    "eval_optim_lrschedulerOnPlateau()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code implémente une fonction eval_nn_optim_lrschedulerOnPlateau pour évaluer la performance de différents optimiseurs comme SGD, RMSProp, Adagrad, Adam, et AdamW sur un problème d'optimisation simple. L'objectif est de minimiser une fonction de perte en ajustant les paramètres d'un réseau de neurones avec un scheduler de taux d'apprentissage LRSchedulerOnPlateau.\n",
    "\n",
    "Durant les 300 itérations d’entraînement pour chaque optimiseur, le réseau effectue les étapes classiques : calcul des prédictions en appliquant une transformation linéaire aux entrées, évaluation de la perte via une fonction d'erreur quadratique moyenne, rétropropagation pour mettre à jour les gradients, et mise à jour des paramètres. Le scheduler ajuste ensuite le taux d'apprentissage en fonction de l'évolution de la perte. Une fois toutes les étapes terminées, la perte finale est enregistrée pour chaque optimiseur.\n",
    "\n",
    "Les résultats montrent des différences significatives entre les optimiseurs avec de bons résultats et de bonnes convergences, on remarque avec un lr > 0.2 environs, il y a de mauvaises valeurs qui apparaissent comme Nan pour SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SGD': 6.711699196415566e-09, 'RMSProp': 4.687627890020973e-12, 'Adagrad': 7.378509326372296e-05, 'Adam': 0.0002457509108353406, 'AdamW': 0.11940786242485046}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'SGD': 6.711699196415566e-09,\n",
       " 'RMSProp': 4.687627890020973e-12,\n",
       " 'Adagrad': 7.378509326372296e-05,\n",
       " 'Adam': 0.0002457509108353406,\n",
       " 'AdamW': 0.11940786242485046}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_nn_optim_lrschedulerOnPlateau():\n",
    "    x = torch.linspace(-1, 1, 100)\n",
    "    y_true = 3 * x + 2\n",
    "\n",
    "    W1 = torch.randn(1, requires_grad=True)\n",
    "    b1 = torch.randn(1, requires_grad=True)\n",
    "    W2 = torch.randn(1, requires_grad=True)\n",
    "    b2 = torch.randn(1, requires_grad=True)\n",
    "\n",
    "    optimizers = {\n",
    "        \"SGD\": SGD([W1, b1, W2, b2], learning_rate=0.1),\n",
    "        \"RMSProp\": RMSProp([W1, b1, W2, b2], learning_rate=0.1),\n",
    "        \"Adagrad\": Adagrad([W1, b1, W2, b2], learning_rate=0.1),\n",
    "        \"Adam\": Adam([W1, b1, W2, b2], learning_rate=0.1),\n",
    "        \"AdamW\": AdamW([W1, b1, W2, b2], learning_rate=0.1, weight_decay=0.01)\n",
    "    }\n",
    "\n",
    "    step = 300\n",
    "    result = {}\n",
    "\n",
    "    for optim_name, optimizer in optimizers.items():\n",
    "        # print(f\"\\nOptimiseur : {optim_name}\")\n",
    "\n",
    "        # Réinitialisation des paramètres\n",
    "        W1.data = torch.randn(1)\n",
    "        b1.data = torch.randn(1)\n",
    "        W2.data = torch.randn(1)\n",
    "        b2.data = torch.randn(1)\n",
    "        \n",
    "        scheduler = LRSchedulerOnPlateau(optimizer, initial_lr=0.2)\n",
    "\n",
    "        for epoch in range(step):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = func_nn(x, W1, b1, W2, b2)\n",
    "\n",
    "            loss = mse(y_true, y_pred).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            scheduler.step(loss.item())\n",
    "\n",
    "            # if (epoch + 1) % 10 == 0:\n",
    "            #     current_lr = scheduler.get_lr()\n",
    "            #     print(f\"Epoch {epoch + 1}: Loss = {loss.item():.4f}, LR = {current_lr}\")\n",
    "\n",
    "        result[optim_name] = loss.item()\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "eval_nn_optim_lrschedulerOnPlateau()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En résumé, ces scripts mettent en évidence l'importance du choix de l'optimiseur et de l'adaptation dynamique du taux d'apprentissage, en particulier dans des scénarios où les caractéristiques des fonctions de perte diffèrent considérablement. Les résultats soulignent l'efficacité d'optimiseurs comme Adagrad et RMSProp lorsqu'ils sont associés à un planificateur robuste comme LrSchedulerOnPlateau, qui permet une convergence rapide et précise même dans des conditions d'optimisation difficiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
